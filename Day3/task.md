## AIDD 30-Day Challenge — Task 3 (Complete Submission)##

Asma Yaseen & Yusra Saleem Class Coordinator 
Under the Supervision of Sir Hamzah Syed

##By: Darakshan Anjum##
##Date : 20-11-2025##

Q 1. What new improvements were introduced in Gemini 3.0?
ANS. Gemini 3 Pro has much stronger *reasoning*, enabling it to think more deeply and handle complicated tasks. 
* It introduces “vibe coding,” where you can describe what you want in natural language and Gemini builds interactive apps for you. 
* There is a *Deep Think* mode that boosts reasoning even more, helping with really challenging or novel problems. 
* Multimodal understanding is much better Gemini 3 can reason about images, documents, video, and spatial layouts. 
* Google gives developers new controls in the API, like a “thinking_level” parameter (which manages how deeply the model thinks) and options to adjust visual fidelity to balance cost and speed. 
* It also has a huge context window (1 million tokens), letting it take in very large amounts of information. 

Q. 2. How does Gemini 3.0 improve coding & automation workflows?
ANS. Gemini 3 Pro is very good at *agentic coding*, meaning it doesn't just help you write code. It plans, executes, and verifies tasks across multiple steps. 
* With “vibe coding,” you can just describe your app or idea in plain language, and Gemini will generate full interactive apps you don’t need to write all the boilerplate. 
* Gemini 3 is integrated into a new development platform called **Google Antigravity**, where AI agents can use the editor, terminal, and browser to work more autonomously. 
* In the Gemini API, there’s a tool that proposes shell commands (bash) which helps automate system tasks, file navigation, and development steps. 
* It also supports structured tool-calling and grounding with Google Search or URLs, which is useful for building agents that fetch and format data for further tasks. 

Q3. How does Gemini 3.0 improve multimodal understanding?
ANS. Gemini 3 has **very strong vision reasoning**: it doesn’t just do OCR (read text in images), but can understand the content of complex documents (like analyzing a layout, reading tables, interpreting diagrams). 
* Its **spatial reasoning** is improved: it can understand spatial relationships, trajectories, and even user interactions on a screen. 
* For video, Gemini 3 can process high-frame-rate content and understand action over time, and even remember information across long video contexts. 
* Because of its 1 million-token context window, it can combine lots of data text, images, video, code all in one “thought process,” making it very powerful for multimodal tasks. 

Q4. Name any two developer tools introduced with Gemini 3.0.

ANS.1. **Google Antigravity** An agentic development platform where AI agents (powered by Gemini 3) can write code, run terminal commands, open browser, and validate their work. 
2. **Gemini CLI** — A command-line interface tool that lets developers use Gemini from their terminal for coding, research, task automation, and more. 

More than **Google AI Studio**  A web-based environment for building AI-native apps by giving prompts to Gemini and iterating visually. 





